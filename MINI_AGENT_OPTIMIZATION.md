# ğŸš€ MINI-AGENT OPTIMIZATION GUIDE
## Make It Fast, Lightweight, and Perfect

---

## ğŸ¯ **1. MODEL SELECTION (CRITICAL)**

### **Current Recommendation: Use Smaller, Specialized Models**

Instead of using GPT-4 or large models for mini-agent, use **small, fast models**:

#### **Best Models for Mini-Agent:**

| Model | Speed | Cost | Quality | Best For |
|-------|-------|------|---------|----------|
| **GPT-3.5-Turbo** | âš¡âš¡âš¡ (Fast) | ğŸ’° (Cheap) | âœ…âœ…âœ… | Production - Perfect balance |
| **Claude Instant** | âš¡âš¡âš¡ (Fast) | ğŸ’° (Cheap) | âœ…âœ…âœ… | Production - Very reliable |
| **Gemini 1.5 Flash** | âš¡âš¡âš¡âš¡ (Fastest) | ğŸ’° (Cheapest) | âœ…âœ…âœ… | Production - Google's fast model |
| **Llama 3 8B** | âš¡âš¡âš¡âš¡ (Local) | Free | âœ…âœ…âœ… | Self-hosted - Privacy |
| GPT-4 | âš¡ (Slow) | ğŸ’°ğŸ’°ğŸ’° (Expensive) | âœ…âœ…âœ…âœ…âœ… | Avoid - Overkill |

### **âœ… RECOMMENDATION:**

```python
# Use Gemini 1.5 Flash for mini-agent
# Fast + Cheap + Good enough for clarifications

MINI_AGENT_MODEL = "gemini-1.5-flash"  # <-- Google's fastest
MAIN_CHAT_MODEL = "gpt-4"  # Keep GPT-4 for main conversation
```

**Why Gemini Flash?**
- âš¡ **2x faster** than GPT-3.5
- ğŸ’° **80% cheaper** than GPT-3.5
- âœ… **Perfect for short explanations**
- ğŸ¯ **Optimized for quick clarifications**

---

## ğŸ”§ **2. PIPELINE OPTIMIZATION**

### **A. Parallel Processing (CRITICAL)**

**Current:** Sequential operations  
**Better:** Parallel database + Redis calls

```python
# âŒ SLOW - Sequential
thread = await mini_agents_collection.find_one({"id": thread_id})
history = await format_mini_agent_history(user_id, message_id)
response = await get_llm_response(prompt, system_prompt)

# âœ… FAST - Parallel
import asyncio

async def add_mini_agent_message(thread_id, request):
    # Fetch thread and history in parallel
    thread_task = mini_agents_collection.find_one({"id": thread_id})
    history_task = format_mini_agent_history(user_id, message_id)
    
    thread, history = await asyncio.gather(thread_task, history_task)
    
    # Now process...
```

**Impact:** 30-50% faster

---

### **B. Reduce Database Calls**

**Current:** Multiple DB calls per request  
**Better:** Batch operations

```python
# âŒ SLOW - 2 separate inserts
await messages_collection.insert_one(user_message_db)
await messages_collection.insert_one(ai_message_db)

# âœ… FAST - Single batch insert
await messages_collection.insert_many([user_message_db, ai_message_db])
```

**Impact:** 2x faster DB operations

---

### **C. Connection Pooling (Already Done âœ…)**

You're already using Redis connection pooling - great!

Make sure MongoDB also uses pooling:

```python
# In mongo_client.py
client = AsyncIOMotorClient(
    MONGODB_URL,
    maxPoolSize=50,  # Connection pool
    minPoolSize=10,
    maxIdleTimeMS=30000
)
```

---

## ğŸ’¡ **3. CACHING STRATEGIES**

### **A. Cache Frequently Asked Questions**

```python
# Redis cache for common snippet explanations
CACHE_KEY = f"MINI_AGENT_CACHE:{hash(snippet_text)}:{hash(question)}"

async def get_cached_response(snippet_text, question):
    cache_key = f"MINI_AGENT_CACHE:{hash(snippet_text)}:{hash(question)}"
    cached = await redis_client.get(cache_key)
    
    if cached:
        logger.info("âœ… Cache hit - returning cached response")
        return json.loads(cached)
    
    return None

async def cache_response(snippet_text, question, response, ttl_hours=24):
    cache_key = f"MINI_AGENT_CACHE:{hash(snippet_text)}:{hash(question)}"
    await redis_client.setex(
        cache_key, 
        ttl_hours * 3600, 
        json.dumps(response)
    )
```

**Impact:**
- âš¡ **Instant responses** for repeated questions
- ğŸ’° **90% cost reduction** on common queries
- ğŸ¯ **No LLM call needed** for cache hits

---

### **B. Snippet Embeddings Cache (Advanced)**

For very fast similar question detection:

```python
# Cache snippet embeddings
EMBEDDING_CACHE = f"EMBEDDING:{hash(snippet_text)}"

# If similar question asked before â†’ return cached answer
# Uses vector similarity instead of exact match
```

---

## ğŸ¯ **4. TOKEN OPTIMIZATION**

### **Current Issue:** Verbose system prompt

Your system prompt is ~400 tokens. Optimize it:

```python
# âŒ VERBOSE (~400 tokens)
system_prompt = """
You are a Mini-Agent â€” a calm, precise clarification tool.

YOUR ROLE:
Explain the selected text clearly and directly. Think of yourself as a margin note...
[All the rules listed out]
"""

# âœ… CONCISE (~150 tokens)
system_prompt = """Mini-Agent: Calm, precise clarifier. Explain selected text in 1-2 sentences.

Rules:
- No greetings/closings
- Neutral tone, no "obviously/simply/just"
- No bullets unless asked
- Build on previous (if shown)
- Never say "earlier I explained"

Failure: "This depends on context. Select more."
"""
```

**Impact:**
- ğŸ’° **60% less tokens** â†’ cheaper
- âš¡ **Faster processing** â†’ quicker
- ğŸ¯ **Same quality** â†’ works perfectly

---

### **Conversation History Limit**

```python
# Current: Unlimited history
# Better: Last 3 clarifications only

context["clarifications"] = context["clarifications"][-3:]  # Was -5
```

**Impact:**
- ğŸ’° **Fewer tokens** per request
- âš¡ **Faster processing**
- âœ… **Still maintains flow**

---

## âš¡ **5. RESPONSE STREAMING (BIG WIN)**

### **Current:** Wait for full response  
**Better:** Stream response as it generates

```python
# In llm_client.py
async def get_llm_response_stream(prompt, system_prompt):
    """Stream LLM response token by token"""
    
    # For OpenAI
    response = await openai.ChatCompletion.create(
        model="gemini-1.5-flash",
        messages=[...],
        stream=True  # âœ… Enable streaming
    )
    
    async for chunk in response:
        if chunk.choices[0].delta.content:
            yield chunk.choices[0].delta.content

# In highlights.py
async def add_mini_agent_message(thread_id, request):
    # Stream response back to frontend
    async for token in get_llm_response_stream(prompt, system_prompt):
        # Send token to frontend via websocket
        await websocket.send_json({"type": "token", "content": token})
```

**Impact:**
- âš¡ **Perceived speed improvement** - user sees response immediately
- ğŸ¯ **Better UX** - feels instant
- âœ… **Actually faster** - doesn't wait for full completion

---

## ğŸ—ï¸ **6. INFRASTRUCTURE OPTIMIZATION**

### **A. Use Edge Functions (If Possible)**

Deploy mini-agent endpoint to edge:

```
Vercel Edge Functions
Cloudflare Workers
AWS Lambda@Edge
```

**Benefits:**
- âš¡ **Lower latency** (closer to users)
- ğŸŒ **Global performance**
- ğŸ’° **Auto-scaling**

---

### **B. Redis Optimization**

```python
# Use Redis pipeline for multiple operations
pipeline = redis_client.pipeline()
pipeline.get(key1)
pipeline.get(key2)
pipeline.set(key3, value3)
results = await pipeline.execute()
```

**Impact:** 3-5x faster than individual calls

---

### **C. MongoDB Indexes**

```python
# Add indexes for fast lookups
await mini_agents_collection.create_index("id")
await mini_agents_collection.create_index("sessionId")
await messages_collection.create_index("threadId")
```

**Impact:** 10-100x faster queries

---

## ğŸ¯ **7. LIGHTWEIGHT CONTEXT**

### **Reduce Context Size**

```python
# Only send what's needed
snippet_context = f"TEXT: {selected_text[:200]}\n"  # Limit to 200 chars

# Instead of full thread data
```

**Impact:**
- ğŸ’° **Fewer tokens**
- âš¡ **Faster processing**
- âœ… **Usually sufficient**

---

## ğŸ§ª **8. PRE-PROCESSING OPTIMIZATIONS**

### **A. Question Classification**

Detect question type and use different strategies:

```python
def classify_question(question):
    """Fast, simple classification"""
    
    lower_q = question.lower()
    
    # Definition questions - use cached embeddings
    if any(word in lower_q for word in ['what is', 'what does', 'define']):
        return 'definition'
    
    # Clarification - check history
    if any(word in lower_q for word in ['why', 'how', 'can you']):
        return 'clarification'
    
    # Example request - might need more tokens
    if 'example' in lower_q or 'instance' in lower_q:
        return 'example'
    
    return 'general'

# Use different models/strategies based on type
if question_type == 'definition':
    # Use ultra-fast model
    model = "gemini-1.5-flash"
elif question_type == 'example':
    # Use slightly better model
    model = "gpt-3.5-turbo"
```

---

### **B. Smart Caching by Question Type**

```python
# Cache definitions longer (they don't change)
if question_type == 'definition':
    cache_ttl = 7 * 24 * 3600  # 7 days
else:
    cache_ttl = 1 * 3600  # 1 hour
```

---

## ğŸ“Š **9. MONITORING & METRICS**

### **Track Performance**

```python
import time

async def add_mini_agent_message(thread_id, request):
    start_time = time.time()
    
    # ... process request ...
    
    # Log metrics
    duration = time.time() - start_time
    logger.info(f"â±ï¸ Mini-agent response time: {duration:.2f}s")
    
    # Alert if slow
    if duration > 3.0:
        logger.warning(f"âš ï¸ Slow mini-agent response: {duration:.2f}s")
```

**Track:**
- Response time
- Cache hit rate
- Token usage
- Error rate

---

## ğŸ¯ **10. COMPLETE OPTIMIZED PIPELINE**

### **Perfect Mini-Agent Architecture:**

```python
async def add_mini_agent_message_OPTIMIZED(thread_id, request):
    start = time.time()
    
    # 1. Check cache first
    cache_key = f"CACHE:{hash(snippet)}:{hash(question)}"
    cached = await redis_client.get(cache_key)
    if cached:
        return json.loads(cached)  # âš¡ Instant return
    
    # 2. Parallel fetch (thread + history)
    thread, history = await asyncio.gather(
        mini_agents_collection.find_one({"id": thread_id}),
        format_mini_agent_history(user_id, message_id)
    )
    
    # 3. Build minimal prompt
    prompt = f"TEXT: {snippet[:200]}\n"
    if history:
        # Only last 2 QA pairs
        recent_history = "\n".join(history.split("\n\n")[-2:])
        prompt += f"RECENT:\n{recent_history}\n"
    prompt += f"Q: {request.text}"
    
    # 4. Use fast model
    response = await get_llm_response_stream(
        prompt=prompt,
        system_prompt=CONCISE_SYSTEM_PROMPT,  # Optimized version
        model="gemini-1.5-flash"  # Fastest model
    )
    
    # 5. Store in batch
    await asyncio.gather(
        messages_collection.insert_many([user_msg, ai_msg]),
        store_mini_agent_context(user_id, message_id, q, a),
        redis_client.setex(cache_key, 3600, json.dumps(response))
    )
    
    logger.info(f"âš¡ Response in {time.time() - start:.2f}s")
    return response
```

---

## ğŸ“‹ **IMPLEMENTATION PRIORITY**

### **Quick Wins (Do First):**

1. âœ… **Switch to Gemini Flash** - 2x faster, cheaper
2. âœ… **Parallel DB calls** - 30-50% faster
3. âœ… **Optimize system prompt** - 60% fewer tokens
4. âœ… **Add response caching** - 90% faster for common Qs
5. âœ… **Batch DB inserts** - 2x faster writes

### **Medium Effort:**

6. âœ… **Add MongoDB indexes** - 10x faster queries
7. âœ… **Reduce history to last 3** - Faster, cheaper
8. âœ… **Question classification** - Smart routing

### **Advanced:**

9. âœ… **Response streaming** - Better UX
10. âœ… **Edge deployment** - Lower latency
11. âœ… **Embedding cache** - Ultra-fast similar Q detection

---

## ğŸ¯ **EXPECTED IMPROVEMENTS**

### **Before Optimization:**
- â±ï¸ Response time: 3-5 seconds
- ğŸ’° Cost per request: $0.002
- ğŸ¯ Cache hit rate: 0%

### **After Optimization:**
- â±ï¸ Response time: **0.5-1.5 seconds** (3-5x faster)
- ğŸ’° Cost per request: **$0.0003** (85% cheaper)
- ğŸ¯ Cache hit rate: **40-60%** (common questions)

---

## ğŸ† **RECOMMENDED STACK**

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  MINI-AGENT OPTIMIZED STACK         â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                     â”‚
â”‚  Model: Gemini 1.5 Flash           â”‚  âš¡ Fastest
â”‚  Cache: Redis (24hr for defs)      â”‚  ğŸ’¾ Smart caching
â”‚  History: Last 3 QA pairs          â”‚  ğŸ¯ Lightweight
â”‚  Prompt: 150 tokens (optimized)    â”‚  ğŸ’° Cheap
â”‚  DB: Batch inserts + indexes       â”‚  ğŸš€ Fast writes
â”‚  Processing: Parallel operations   â”‚  âš¡ Concurrent
â”‚  Streaming: Token-by-token         â”‚  ğŸ‘ï¸ Instant feel
â”‚                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ’¡ **BONUS: MODEL FALLBACK STRATEGY**

```python
# Try fast model first, fallback to better if unsure
async def get_smart_response(prompt, system_prompt):
    # Try Gemini Flash (fast & cheap)
    response = await get_llm_response(
        prompt, 
        system_prompt, 
        model="gemini-1.5-flash"
    )
    
    # Check confidence (you can add confidence scoring)
    if is_low_confidence(response):
        # Fallback to GPT-3.5
        response = await get_llm_response(
            prompt,
            system_prompt,
            model="gpt-3.5-turbo"
        )
    
    return response
```

---

## âœ… **ACTION ITEMS**

**Start with these 5 changes:**

1. **Switch model to Gemini Flash**
   - Update `get_llm_response()` call
   - Add model parameter

2. **Optimize system prompt**
   - Reduce to ~150 tokens
   - Keep same rules, compress wording

3. **Add response caching**
   - Cache common questions for 24 hours
   - Use snippet+question hash as key

4. **Parallel DB operations**
   - Use `asyncio.gather()` for concurrent calls
   - Combine MongoDB inserts

5. **Add MongoDB indexes**
   - Index `id`, `sessionId`, `threadId`
   - Massive query speedup

**Expected result:** 3-4x faster, 80% cheaper, same quality âœ…

---

**Status:** Ready to implement  
**Effort:** 2-4 hours total  
**Impact:** Massive performance improvement  
**Risk:** Low (all proven optimizations)
